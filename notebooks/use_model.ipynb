{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cf3ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed7dffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"../models\"\n",
    "pca_models_path = models_path + \"/pca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dda56aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(landmarks: list) -> list:\n",
    "    \"\"\"Normalize a list of pose landmarks.\n",
    "\n",
    "    This function brings the center (mean of outer most x, y and z) to (0, 0, 0)\n",
    "    and scales so that the maximum distance from the center is 0.5.\n",
    "    Also remove the z coordinate.\n",
    "\n",
    "    Args:\n",
    "        landmarks (list): A flat list of landmark values [x1, y1, z1, v1, x2, y2, z2, v2, ..., xN, yN, zN, vN].\n",
    "\n",
    "    Returns:\n",
    "        list: The normalized landmark list in the same format.\n",
    "\n",
    "    \"\"\"\n",
    "    landmarks = np.array(landmarks).reshape(-1, 4)\n",
    "    max_x = np.max(landmarks[:, 0])\n",
    "    min_x = np.min(landmarks[:, 0])\n",
    "    max_y = np.max(landmarks[:, 1])\n",
    "    min_y = np.min(landmarks[:, 1])\n",
    "\n",
    "    # Get center\n",
    "    center_x = (max_x + min_x) / 2\n",
    "    center_y = (max_y + min_y) / 2\n",
    "    \n",
    "    # Bring center to (0, 0, 0)\n",
    "    landmarks[:, 0] -= center_x\n",
    "    landmarks[:, 1] -= center_y\n",
    "\n",
    "    # Get max distance from center\n",
    "    max_distance = np.max(np.sqrt(landmarks[:, 0]**2 + landmarks[:, 1]**2 + landmarks[:, 2]**2))\n",
    "\n",
    "    # Scale to 0.5\n",
    "    scale = 0.5 / max_distance\n",
    "    landmarks[:, 0] *= scale\n",
    "    landmarks[:, 1] *= scale\n",
    "\n",
    "    # Remove z coordinate\n",
    "    landmarks = landmarks[:, :2]\n",
    "\n",
    "    # Flatten the array and convert to list\n",
    "    landmarks = landmarks.flatten().tolist()\n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb0e459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_classify_image(image_path, model, pose, pca_model=None):\n",
    "    \"\"\"Extract pose landmarks from an image and classify it using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        model: Pre-trained model for classification.\n",
    "        pose: Pre-trained pose detector.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect pose landmarks\n",
    "    results = pose.process(image_rgb)\n",
    "    if results.pose_landmarks:\n",
    "        # Extract landmarks\n",
    "        landmarks = []\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "        # Normalize landmarks\n",
    "        landmarks = normalize_landmarks(landmarks)\n",
    "\n",
    "        if pca_model:\n",
    "            landmarks = pca_model.transform([landmarks])[0]\n",
    "\n",
    "        # Classify the image using the extracted landmarks\n",
    "        landmarks_array = np.array(landmarks).reshape(1, -1)\n",
    "        classification_result = model.predict(landmarks_array)\n",
    "\n",
    "        return classification_result\n",
    "    else:\n",
    "        print(\"No pose landmarks detected in the image.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "976892b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747106144.915100 1428031 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1747106144.925153 4139957 gl_context.cc:369] GL version: 3.0 (OpenGL ES 3.0 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: D3D12 (Intel(R) UHD Graphics 770)\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54fa9ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747106145.002763 4139935 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1747106145.058849 4139934 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PCA model from ../models/pca/pca.joblib\n",
      "Loaded PCA model from ../models/pca/MLPClassifier_pca.joblib\n",
      "Loaded PCA model from ../models/pca/knn_pca.joblib\n",
      "Image: watering_test_2.jpg, PCA Model: MLPClassifier_pca.joblib, Classification Result: ['fertilizing']\n",
      "Image: watering_test_2.jpg, PCA Model: knn_pca.joblib, Classification Result: ['fertilizing']\n",
      "\n",
      "Image: hoeing_test_3.png, PCA Model: MLPClassifier_pca.joblib, Classification Result: ['checking']\n",
      "Image: hoeing_test_3.png, PCA Model: knn_pca.joblib, Classification Result: ['checking']\n",
      "\n",
      "Image: hoeing_test_1.jpg, PCA Model: MLPClassifier_pca.joblib, Classification Result: ['hoeing']\n",
      "Image: hoeing_test_1.jpg, PCA Model: knn_pca.joblib, Classification Result: ['hoeing']\n",
      "\n",
      "Image: watering_test.jpg, PCA Model: MLPClassifier_pca.joblib, Classification Result: ['fertilizing']\n",
      "Image: watering_test.jpg, PCA Model: knn_pca.joblib, Classification Result: ['fertilizing']\n",
      "\n",
      "Image: checking_test_1.jpg, PCA Model: MLPClassifier_pca.joblib, Classification Result: ['checking']\n",
      "Image: checking_test_1.jpg, PCA Model: knn_pca.joblib, Classification Result: ['checking']\n",
      "\n",
      "Image: hoeing_test_2.jpg, PCA Model: MLPClassifier_pca.joblib, Classification Result: ['planting']\n",
      "Image: hoeing_test_2.jpg, PCA Model: knn_pca.joblib, Classification Result: ['hoeing']\n",
      "\n",
      "\n",
      "Image: checking_test_2.jpg, PCA Model: MLPClassifier_pca.joblib, Classification Result: ['fertilizing']\n",
      "Image: checking_test_2.jpg, PCA Model: knn_pca.joblib, Classification Result: ['prunning']\n",
      "\n",
      "Image: checking_test_3.jpg, PCA Model: MLPClassifier_pca.joblib, Classification Result: ['checking']\n",
      "Image: checking_test_3.jpg, PCA Model: knn_pca.joblib, Classification Result: ['checking']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pca models\n",
    "pca_models = {}\n",
    "for file in os.listdir(pca_models_path):\n",
    "    if file.endswith(\".joblib\"):\n",
    "        model_path = os.path.join(pca_models_path, file)\n",
    "        model = joblib.load(model_path)\n",
    "        pca_models[file] = model\n",
    "        print(f\"Loaded PCA model from {model_path}\")\n",
    "pca_model = pca_models.pop(\"pca.joblib\")\n",
    "for file in os.listdir(\"../test_data\"):\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "        image_path = os.path.join(\"../test_data\", file)\n",
    "        for name, model in pca_models.items():\n",
    "            result = extract_features_and_classify_image(image_path, model, pose, pca_model)\n",
    "            if result is not None:\n",
    "                print(f\"Image: {file}, PCA Model: {name}, Classification Result: {result}\")\n",
    "            else:\n",
    "                print(f\"Image: {file}, PCA Model: {name}, No landmarks detected.\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c150ae9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Detect pose landmarks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m results = \u001b[43mpose\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results.pose_landmarks:\n\u001b[32m     17\u001b[39m     landmarks = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/pose-detection-in-cultivating/.venv/lib/python3.12/site-packages/mediapipe/python/solutions/pose.py:185\u001b[39m, in \u001b[36mPose.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    165\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[32m    166\u001b[39m \n\u001b[32m    167\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m   results = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m results.pose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results.pose_landmarks.landmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/pose-detection-in-cultivating/.venv/lib/python3.12/site-packages/mediapipe/python/solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Process all video files in the test_data directory and classify each frame\n",
    "video_extensions = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
    "for video_file in os.listdir(\"../test_data\"):\n",
    "    if video_file.lower().endswith(video_extensions):\n",
    "        video_path = os.path.join(\"../test_data\", video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_idx = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Convert frame to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Detect pose landmarks\n",
    "            results = pose.process(frame_rgb)\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = []\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "            #     landmarks = normalize_landmarks(landmarks)\n",
    "            #     landmarks = pca_model.transform([landmarks])[0]\n",
    "            #     landmarks_array = np.array(landmarks).reshape(1, -1)\n",
    "            #     classification_result = model.predict(landmarks_array)\n",
    "            #     print(f\"Video: {video_file}, Frame: {frame_idx}, Classification Result: {classification_result}\")\n",
    "            # else:\n",
    "            #     print(f\"Video: {video_file}, Frame: {frame_idx}, No landmarks detected.\")\n",
    "            # frame_idx += 1\n",
    "            # Draw landmarks on the frame and save to video file\n",
    "        cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
